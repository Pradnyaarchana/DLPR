{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c546a66-4ba3-42e1-af54-b1492fa4267e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Pradnya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Pradnya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 1: Import Libraries !pip install nltk gensim\n",
    "import nltk, re, gensim\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('punkt') #'punkt' — sentence/word tokenizer models.\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "118eb3a3-27e5-493e-87af-6cdfa1bdbeb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gensim chosen because it has an efficient Word2Vec implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7e6a82ef-34fc-4d8e-ac32-3b53c0607ecc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Pradnya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "493b0d4c-ce53-485f-9a91-a6a584d18a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Input Paragraph\n",
    "text = \"\"\"\"I love playing football with my friends. \n",
    "We often meet at the ground in the evening. \n",
    "After playing, we talk about our favorite teams and players. \n",
    "Sports help us stay healthy and energetic.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e92dd4da-ba59-4513-b3f2-b1596f42c69c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Preprocessing\n",
    "text = text.lower()\n",
    "sentences = nltk.sent_tokenize(text)\n",
    "cleaned = []\n",
    "\n",
    "for s in sentences:\n",
    "    s = re.sub('[^a-z0-9 ]+', '', s)   # remove special characters\n",
    "    cleaned.append(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "981979cd-1ced-447b-98c6-762e68e6a32b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned Tokens: [['love', 'playing', 'football', 'friends'], ['often', 'meet', 'ground', 'evening'], ['playing', 'talk', 'favorite', 'teams', 'players'], ['sports', 'help', 'us', 'stay', 'healthy', 'energetic']]\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Tokenization and Stopword Removal\n",
    "tokens = [nltk.word_tokenize(s) for s in cleaned]\n",
    "sw = set(stopwords.words('english'))\n",
    "for i in range(len(tokens)):\n",
    "    tokens[i] = [w for w in tokens[i] if w not in sw]\n",
    "\n",
    "print(\"Cleaned Tokens:\", tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fcb1ccb1-0d42-48f7-b6e6-bcedbd6f19ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Train Word2Vec Model (CBOW)\n",
    "model = gensim.models.Word2Vec(tokens, vector_size=50, window=5, min_count=1, sg=0)\n",
    "\n",
    "\n",
    "# min_count — setting to 1 includes rare tokens (might add noise). In real training, set min_count to 2–5 to ignore typos/rare tokens.\n",
    "# CBOW predicts a target word from its surrounding context (averaging context vector to predict target). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f125bea3-cd21-489a-bb11-9466c2f79808",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example context-target pairs:\n",
      "Context: ['playing', 'talk', 'teams', 'players'] → Target: favorite\n",
      "Context: ['sports', 'help', 'stay', 'healthy'] → Target: us\n",
      "Context: ['help', 'us', 'healthy', 'energetic'] → Target: stay\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Generate Context–Target Pairs (for reference)\n",
    "data = []  # List to store (context, target) pairs\n",
    "window_size = 2  # Two words before and after\n",
    "\n",
    "for sentence in tokens:\n",
    "    if len(sentence) < 2 * window_size + 1:\n",
    "        # Skip short sentences that don't have enough words for full context\n",
    "        continue\n",
    "\n",
    "    # Loop through each word, skipping the first and last two words\n",
    "    for i in range(window_size, len(sentence) - window_size):\n",
    "        # Context: two words before + two words after\n",
    "        context = [\n",
    "            sentence[i - 2],\n",
    "            sentence[i - 1],\n",
    "            sentence[i + 1],\n",
    "            sentence[i + 2]\n",
    "        ]\n",
    "        # Target: the current word\n",
    "        target = sentence[i]\n",
    "\n",
    "        data.append((context, target))\n",
    "\n",
    "# Display some context-target examples\n",
    "print(\"\\nExample context-target pairs:\")\n",
    "for i in range(3):\n",
    "    print(f\"Context: {data[i][0]} → Target: {data[i][1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71dda29d-b76c-4838-b174-7eed6cdc1935",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "085f0aa8-1a5d-4ef4-97f5-0201b0d553ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8084c8-7e92-4dab-ada3-42048927914e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4bb5be8-4d27-43ed-82a9-0102d899f6a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d02f7a4-9ad5-43d8-8e58-be8826705d1b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
